{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that I have a good idea of the dataset I always start by a state of the art and the problem definition.\n",
    "\n",
    "I used https://www.notion.so/ to document my state of the art. I have a template that I always use to document my research.\n",
    "You can find in it :\n",
    "- Model / architecture selection\n",
    "- Hyper-parameters\n",
    "- Rough description of the data (origin, size, date, features…)\n",
    "- Results (ie: precision, recall, f1…).\n",
    "- A link to a snapshot of data (if possible)\n",
    "- Commentary and learning\n",
    "- ...\n",
    "\n",
    "The Problem definition is really important to evaluate the client's needs (Data Availability + Accuracy requirement + Problem difficulty).\n",
    "- Understand how it will benefit the business.\n",
    "- Once you get an idea and you determine business compatibility, you need to define a success metric. Is it 90% accuracy or 95% accuracy or 99% accuracy.\n",
    "- Does your model need to work in realtime?\n",
    "- The project should have high impact, where cheap prediction is valuable for the complex parts of your business process.\n",
    "- The project should have high feasibility, which is driven by the data availability, accuracy requirements, and problem difficulty.\n",
    "- The result should be archived by a human. If a human can't do it, the machine will not too.\n",
    "- Make a Checklist for the project and plan your time.\n",
    "\n",
    "For the state of the art model, you can look on paperswithcode and use frameworks like mmdetection, mmtracking, mmsegmentation or detectron2 to test them easily.\n",
    "=> In actual industry you don’t need crazy perfect metrics. You just need to build things so they can be used and upgraded and then you slowly improve once the infra is in place and working.\n",
    "=> But don't forget, the first thing to do is to have a model which works as good as the actual state => Improve it after.\n",
    "\n",
    "When I can't find a model that fits my needs, I try to get inspiration from the different models to build my own.\n",
    "I always start with a simple architecture :\n",
    "- If you don't find a pre-trained model that could work\n",
    "   - If your data looks like images, start with a LeNet-like architecture and consider using something like ResNet as your codebase gets more mature.\n",
    "   - If your data looks like sequences, start with an LSTM with one hidden layer and/or temporal/classical convolutions. Then, when your problem gets more mature, you can move to an Attention-based model or a WaveNet-like model.\n",
    "   - For all other tasks, start with a fully-connected neural network with one hidden layer and use more advanced networks later depending on the problem.\n",
    "- Try to start with a basic algorithm machine learning, not neural network (XgBoost, RandomForest, simple algorithm...)\n",
    "- Frame all problems as binary classification\n",
    "- Build complicated data pipelines later. These are important for large-scale ML systems, but you should not start with them because data pipelines themselves can be a big source of bugs. Just start with a dataset that you can load into memory.\n",
    "=> I always try to get my model first. I try to overfit a single batch to spot if I have errors in my code.\n",
    "=> You need 10/15 epochs to know if your network is bad or good, don't change anything before that. After that, you can analyse your loss to spot problems and see where the model is bad to maybe add data (Spot underfitting/Overfitting).\n",
    "\n",
    "For Hyper parameters :\n",
    "- I select sensible hyper-parameter defaults :\n",
    "   - Common choices of learning rates are normally in the range α = 0.1, 0.01, 0.001.\n",
    "       - Adam optimizer with a “magic” learning rate value of 3e-4.\n",
    "   - ReLU activation for fully-connected and convolutional models and Tanh activation for LSTM models.\n",
    "   - He initialization for ReLU activation function and Glorot initialization for Tanh activation function.\n",
    "   - No regularization and data normalization.\n",
    "   - Typical batch sizes include 32, 64, 128, and 256\n",
    "   - SGD with momentum or Mini batch gradient descent on big dataset - Nesterov acceleration on small dataset\n",
    "        - The momentum term γ is commonly set to 0.9\n",
    "   - Add a Regularization penalty.\n",
    "   - Implement Learning rate schedulers to increase classification accuracy.\n",
    "- Simplify the problem:\n",
    "    - Working with a small training set around 10,000 examples.\n",
    "    - Using a fixed number of objects, classes, input size, etc.\n",
    "- Evaluate Rank 1 and Rank 5 accuracy\n",
    "- Choose a simple performance metric, but only one ! Try to find one which corresponds to your business and the objective of the model.\n",
    "    - Classification :\n",
    "        - Accuracy - If the classes are well balanced in the dataset like 50/50\n",
    "        - Precision (P) - If the classes are not well balanced in the dataset like 10/90\n",
    "        - Recall (R)\n",
    "        - F1 Score (F1)\n",
    "        - Area under the ROC curve or simple AUC\n",
    "        - Log loss\n",
    "        - Precision at k (P@k)\n",
    "        - Average precision at k (AP@k)\n",
    "        - Mean average precision at k (MAP@k)\n",
    "    - Regression :\n",
    "        - Mean Absolute Error (MAE)\n",
    "        - Mean Squared Error (MSE)\n",
    "        - Root mean squared error (RMSE)\n",
    "        - Root mean squared logarithmic error (RMSLE)\n",
    "        - Mean percentage error (MPE)\n",
    "        - Mean absolute percentage error (MAPE)\n",
    "        - R²\n",
    "\n",
    "I then Setup my tools :\n",
    "- I use DVC to manage my dataset\n",
    "- Weight & Biases to track my experiments and to tune my hyper parameters.\n",
    "- Ray to distribute my training\n",
    "\n",
    "At the end, to improve your models you can :\n",
    "   - If you can, add data + Take care that your dataset is well balanced and you doesn't have errors (Treat missing and Outlier values)\n",
    "   - If the training is too slow :\n",
    "        - Try to reduce I/O latency, transform first dataset into a HDF5 dataset. It will helps loading faster the data.\n",
    "   - Feature Engineering\n",
    "        - Feature engineering is highly influenced by hypotheses generation. Good hypothesis result in good features. That’s why, I always suggest to invest quality time in hypothesis generation. Feature engineering process can be divided into two steps :\n",
    "            - Feature transformation: There are various scenarios where feature transformation is required:\n",
    "               - Changing the scale of a variable from original scale to scale between zero and one.\n",
    "               - Some algorithms works well with normally distributed data. Therefore, we must remove skewness of variable(s).\n",
    "               - Some times, creating bins of numeric data works well, since it handles the outlier values also. Numeric data can be made discrete by grouping values into bins. This is known as data discretization.\n",
    "            - Feature Creation: Deriving new variable(s) from existing variables is known as feature creation. It helps to unleash the hidden relationship of a data set.\n",
    "   - Feature Selection\n",
    "        - Feature Selection is a process of finding out the best subset of attributes which better explains the relationship of independent variables with target variable.\n",
    "           - You can select the useful features based on various metrics like:\n",
    "                - Domain Knowledge: Based on domain experience, we select feature(s) which may have higher impact on target variable.\n",
    "                - Visualization: As the name suggests, it helps to visualize the relationship between variables, which makes your variable selection process easier.\n",
    "   - Ensemble methods - This is the most common approach found majorly in winning solutions of Data science competitions. This technique simply combines the result of multiple weak models and produce better results. This can be achieved through many ways:\n",
    "        - Bagging (Bootstrap Aggregating)\n",
    "        - Boosting\n",
    "   - To be sure of the results you can use :\n",
    "        - Blending\n",
    "        - Stacking\n",
    "   - Tune hyper parameters at the end :\n",
    "        - Learning rate : High\n",
    "        - Learning rate schedule : High\n",
    "        - Loss function : High\n",
    "        - Layer size : High\n",
    "        - Weight initialization : Medium\n",
    "        - Model depth : Medium\n",
    "        - Layer params : Medium\n",
    "        - Weight of regularization : Medium\n",
    "        - Optimizer choice : Low\n",
    "        - Other optimizer params : Low\n",
    "        - Batch size : Low\n",
    "        - Nonlinearity : Low\n",
    "\n",
    "\n",
    "Then When I have a good model for production :\n",
    "- I use Triton to maximize my gpus power\n",
    "- I convert my model to TensorRT (I convert it first to ONXX because I'm using Pytorch)\n",
    "- I reduce fault precision to have better performance (FP32 => FP16 => FP8).\n",
    "- Switch numpy or pandas to rapids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "   Weeks   FVC  Percent  Age  Sex  SmokingStatus\n0     -4  2315    58.25   79    1              1\n1      5  2214    55.71   79    1              1\n2      7  2061    51.86   79    1              1\n3      9  2144    53.95   79    1              1\n4     11  2069    52.06   79    1              1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-4</td>\n      <td>2315</td>\n      <td>58.25</td>\n      <td>79</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>2214</td>\n      <td>55.71</td>\n      <td>79</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>2061</td>\n      <td>51.86</td>\n      <td>79</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>2144</td>\n      <td>53.95</td>\n      <td>79</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>2069</td>\n      <td>52.06</td>\n      <td>79</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./data/osic/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/osic/test.csv\")\n",
    "\n",
    "# We will try to train a model only on Weeks FVC Percent Age Sex and Smoking Status\n",
    "# Drop Patient column\n",
    "train_df = train_df.drop(\"Patient\", 1)\n",
    "test_df = test_df.drop(\"Patient\", 1)\n",
    "\n",
    "# Convert Sex and Smoking status using label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_df['Sex'] = le.fit_transform(train_df['Sex'])\n",
    "test_df['Sex'] = le.transform(test_df['Sex'])\n",
    "\n",
    "train_df['SmokingStatus'] = le.fit_transform(train_df['SmokingStatus'])\n",
    "test_df['SmokingStatus'] = le.transform(test_df['SmokingStatus'])\n",
    "\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Try to find the best model using lazypredict\n",
    "clf = LazyRegressor(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(train_df.drop(\"FVC\", 1), test_df.drop(\"FVC\", 1), train_df[\"FVC\"], test_df[\"FVC\"])\n",
    "\n",
    "print(models)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:05<00:00,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Adjusted R-Squared  R-Squared    RMSE  \\\n",
      "Model                                                                  \n",
      "KernelRidge                                140.04     -33.76 2723.52   \n",
      "MLPRegressor                                87.02     -20.51 2142.23   \n",
      "LinearSVR                                   35.11      -7.53 1348.98   \n",
      "AdaBoostRegressor                            6.25      -0.31  529.03   \n",
      "RANSACRegressor                              6.06      -0.27  519.78   \n",
      "OrthogonalMatchingPursuit                    5.90      -0.22  511.11   \n",
      "OrthogonalMatchingPursuitCV                  5.82      -0.21  507.28   \n",
      "LassoLarsCV                                  5.77      -0.19  504.43   \n",
      "LarsCV                                       5.77      -0.19  504.43   \n",
      "PassiveAggressiveRegressor                   5.76      -0.19  504.03   \n",
      "LassoCV                                      5.76      -0.19  503.98   \n",
      "SGDRegressor                                 5.75      -0.19  503.24   \n",
      "LassoLarsIC                                  5.75      -0.19  503.23   \n",
      "Lasso                                        5.75      -0.19  503.20   \n",
      "TransformedTargetRegressor                   5.74      -0.19  503.10   \n",
      "LinearRegression                             5.74      -0.19  503.10   \n",
      "Lars                                         5.74      -0.19  503.10   \n",
      "BayesianRidge                                5.74      -0.19  503.08   \n",
      "RidgeCV                                      5.74      -0.19  503.08   \n",
      "Ridge                                        5.74      -0.19  503.08   \n",
      "PoissonRegressor                             5.74      -0.18  502.64   \n",
      "HuberRegressor                               5.69      -0.17  500.38   \n",
      "LassoLars                                    5.69      -0.17  499.98   \n",
      "GammaRegressor                               5.61      -0.15  495.71   \n",
      "ElasticNet                                   5.57      -0.14  493.78   \n",
      "ElasticNetCV                                 5.55      -0.14  492.88   \n",
      "GeneralizedLinearRegressor                   5.46      -0.11  487.53   \n",
      "TweedieRegressor                             5.46      -0.11  487.53   \n",
      "NuSVR                                        5.28      -0.07  477.70   \n",
      "SVR                                          5.27      -0.07  477.32   \n",
      "DummyRegressor                               5.16      -0.04  470.85   \n",
      "GradientBoostingRegressor                    4.13       0.22  408.37   \n",
      "HistGradientBoostingRegressor                2.44       0.64  276.76   \n",
      "LGBMRegressor                                2.39       0.65  272.14   \n",
      "KNeighborsRegressor                          2.26       0.69  259.17   \n",
      "GaussianProcessRegressor                     1.80       0.80  206.60   \n",
      "RandomForestRegressor                        1.32       0.92  130.23   \n",
      "BaggingRegressor                             1.25       0.94  115.24   \n",
      "XGBRegressor                                 1.20       0.95  104.32   \n",
      "DecisionTreeRegressor                        1.00       1.00    0.00   \n",
      "ExtraTreesRegressor                          1.00       1.00    0.00   \n",
      "ExtraTreeRegressor                           1.00       1.00    0.00   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "KernelRidge                          0.12  \n",
      "MLPRegressor                         3.28  \n",
      "LinearSVR                            0.01  \n",
      "AdaBoostRegressor                    0.09  \n",
      "RANSACRegressor                      0.02  \n",
      "OrthogonalMatchingPursuit            0.01  \n",
      "OrthogonalMatchingPursuitCV          0.01  \n",
      "LassoLarsCV                          0.01  \n",
      "LarsCV                               0.02  \n",
      "PassiveAggressiveRegressor           0.01  \n",
      "LassoCV                              0.05  \n",
      "SGDRegressor                         0.02  \n",
      "LassoLarsIC                          0.01  \n",
      "Lasso                                0.01  \n",
      "TransformedTargetRegressor           0.01  \n",
      "LinearRegression                     0.01  \n",
      "Lars                                 0.01  \n",
      "BayesianRidge                        0.01  \n",
      "RidgeCV                              0.01  \n",
      "Ridge                                0.01  \n",
      "PoissonRegressor                     0.01  \n",
      "HuberRegressor                       0.01  \n",
      "LassoLars                            0.01  \n",
      "GammaRegressor                       0.01  \n",
      "ElasticNet                           0.01  \n",
      "ElasticNetCV                         0.05  \n",
      "GeneralizedLinearRegressor           0.02  \n",
      "TweedieRegressor                     0.01  \n",
      "NuSVR                                0.12  \n",
      "SVR                                  0.11  \n",
      "DummyRegressor                       0.01  \n",
      "GradientBoostingRegressor            0.12  \n",
      "HistGradientBoostingRegressor        0.31  \n",
      "LGBMRegressor                        0.05  \n",
      "KNeighborsRegressor                  0.01  \n",
      "GaussianProcessRegressor             0.20  \n",
      "RandomForestRegressor                0.26  \n",
      "BaggingRegressor                     0.04  \n",
      "XGBRegressor                         0.05  \n",
      "DecisionTreeRegressor                0.01  \n",
      "ExtraTreesRegressor                  0.19  \n",
      "ExtraTreeRegressor                   0.01  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When the results are not good I use a simple architecture."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# Basic data transformation\n",
    "def transform_df(dataframe):\n",
    "    dataframe.drop_duplicates(keep=False,inplace=True,subset=['Patient','Weeks'])\n",
    "    dataframe['Weeks'] = dataframe['Weeks'].astype(int)\n",
    "    dataframe['min_week'] = dataframe.groupby('Patient')['Weeks'].transform('min')\n",
    "    dataframe['baseline_week'] = dataframe['Weeks'] - dataframe['min_week']\n",
    "    base_df = dataframe.loc[dataframe.Weeks == dataframe.min_week][['Patient','FVC']].copy()\n",
    "    base_df.columns = ['Patient','base_FVC']\n",
    "\n",
    "    base_df['nb']=1\n",
    "    base_df['nb'] = base_df.groupby('Patient')['nb'].transform('cumsum')\n",
    "\n",
    "    base_df = base_df[base_df.nb==1]\n",
    "    base_df.drop('nb',axis =1,inplace=True)\n",
    "    df = dataframe.merge(base_df,on=\"Patient\",how='left')\n",
    "    df.drop(['min_week'], axis = 1)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "                     Patient  Weeks   FVC  Percent  Age   Sex SmokingStatus  \\\n0  ID00007637202177411956430     -4  2315    58.25   79  Male     Ex-smoker   \n1  ID00007637202177411956430      5  2214    55.71   79  Male     Ex-smoker   \n2  ID00007637202177411956430      7  2061    51.86   79  Male     Ex-smoker   \n3  ID00007637202177411956430      9  2144    53.95   79  Male     Ex-smoker   \n4  ID00007637202177411956430     11  2069    52.06   79  Male     Ex-smoker   \n\n   min_week  baseline_week  base_FVC  \n0        -4              0      2315  \n1        -4              9      2315  \n2        -4             11      2315  \n3        -4             13      2315  \n4        -4             15      2315  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n      <th>min_week</th>\n      <th>baseline_week</th>\n      <th>base_FVC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00007637202177411956430</td>\n      <td>-4</td>\n      <td>2315</td>\n      <td>58.25</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>-4</td>\n      <td>0</td>\n      <td>2315</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00007637202177411956430</td>\n      <td>5</td>\n      <td>2214</td>\n      <td>55.71</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>-4</td>\n      <td>9</td>\n      <td>2315</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00007637202177411956430</td>\n      <td>7</td>\n      <td>2061</td>\n      <td>51.86</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>-4</td>\n      <td>11</td>\n      <td>2315</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00007637202177411956430</td>\n      <td>9</td>\n      <td>2144</td>\n      <td>53.95</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>-4</td>\n      <td>13</td>\n      <td>2315</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00007637202177411956430</td>\n      <td>11</td>\n      <td>2069</td>\n      <td>52.06</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>-4</td>\n      <td>15</td>\n      <td>2315</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./data/osic/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/osic/test.csv\")\n",
    "\n",
    "train_df = transform_df(train_df)\n",
    "test_df = transform_df(test_df)\n",
    "\n",
    "train_columns = ['baseline_week','base_FVC','Percent','Age','Sex','SmokingStatus']\n",
    "train_label = ['FVC']\n",
    "sub_columns = ['Patient_Week','FVC','Confidence']\n",
    "\n",
    "train = train_df[train_columns]\n",
    "test = test_df[train_columns]\n",
    "\n",
    "# Pre processing\n",
    "transformer = ColumnTransformer([('s',StandardScaler(),[0,1,2,3]),('o',OneHotEncoder(),[4,5])])\n",
    "target = train_df[train_label].values\n",
    "train = transformer.fit_transform(train)\n",
    "test = transformer.transform(test)\n",
    "\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# Simple Pytorch Model as I said in the introduction of this file\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,n):\n",
    "        super(Model,self).__init__()\n",
    "        self.layer1 = nn.Linear(n,200)\n",
    "        self.layer2 = nn.Linear(200,100)\n",
    "        self.out1 = nn.Linear(100,3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.out2 = nn.Linear(100,3)\n",
    "\n",
    "    def forward(self,xb):\n",
    "        x1 =  F.leaky_relu(self.layer1(xb))\n",
    "        x1 =  F.leaky_relu(self.layer2(x1))\n",
    "        o1 = self.out1(x1)\n",
    "        o2 = F.relu(self.out2(x1))\n",
    "        return o1 + torch.cumsum(o2,dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def score(outputs, target):\n",
    "    confidence = outputs[:,2] - outputs[:,0]\n",
    "    clip = torch.clamp(confidence, min=70)\n",
    "    target = torch.reshape(target, outputs[:,1].shape)\n",
    "    delta = torch.abs(outputs[:, 1] - target)\n",
    "    delta = torch.clamp(delta,max=1000)\n",
    "    sqrt_2 = torch.sqrt(torch.tensor([2.])).to(device)\n",
    "    metrics = (delta*sqrt_2/clip) + torch.log(clip*sqrt_2)\n",
    "    return torch.mean(metrics)\n",
    "\n",
    "def qloss(outputs, target):\n",
    "    qs = [0.25,0.5,0.75]\n",
    "    qs = torch.tensor(qs,dtype=torch.float).to(device)\n",
    "    e =  target - outputs\n",
    "    e.to(device)\n",
    "    v = torch.max(qs*e,(qs-1)*e)\n",
    "    v = torch.sum(v,dim=1)\n",
    "    return torch.mean(v)\n",
    "\n",
    "def loss_fn(outputs, target, l):\n",
    "    return l * qloss(outputs,target) + (1- l) * score(outputs,target)\n",
    "\n",
    "def train_loop(train_loader, model, loss_fn, device, optimizer, lr_scheduler=None):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "    metrics = list()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            metric = score(outputs,labels)\n",
    "\n",
    "            loss = loss_fn(outputs,labels,0.8)\n",
    "            metrics.append(metric.cpu().detach().numpy())\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            if lr_scheduler != None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    return losses,metrics\n",
    "\n",
    "def valid_loop(valid_loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    metrics = list()\n",
    "    for i, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        metric = score(outputs,labels)\n",
    "\n",
    "        loss = loss_fn(outputs,labels,0.8)\n",
    "        metrics.append(metric.cpu().detach().numpy())\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "    return losses,metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n",
      "Fold 0\n",
      "epoch:0 Training | loss:1285.89990234375 score: 10.816282272338867| \n",
      " Validation | loss:331.4751281738281 score:7.497641086578369|\n",
      "epoch:5 Training | loss:134.091552734375 score: 6.514596939086914| \n",
      " Validation | loss:135.57754516601562 score:6.515960693359375|\n",
      "epoch:10 Training | loss:126.68991088867188 score: 6.451584815979004| \n",
      " Validation | loss:131.455078125 score:6.479407787322998|\n",
      "epoch:15 Training | loss:126.3822021484375 score: 6.449957847595215| \n",
      " Validation | loss:131.31826782226562 score:6.478060722351074|\n",
      "cuda is used\n",
      "Fold 1\n",
      "epoch:0 Training | loss:1388.9141845703125 score: 10.927675247192383| \n",
      " Validation | loss:698.1372680664062 score:8.32965087890625|\n",
      "epoch:5 Training | loss:138.52481079101562 score: 6.59238338470459| \n",
      " Validation | loss:138.4564208984375 score:6.589868545532227|\n",
      "epoch:10 Training | loss:133.8900146484375 score: 6.552117347717285| \n",
      " Validation | loss:135.2467498779297 score:6.561423301696777|\n",
      "epoch:15 Training | loss:133.54908752441406 score: 6.548689842224121| \n",
      " Validation | loss:135.10536193847656 score:6.560259819030762|\n",
      "cuda is used\n",
      "Fold 2\n",
      "epoch:0 Training | loss:1282.9613037109375 score: 10.670610427856445| \n",
      " Validation | loss:437.228271484375 score:7.855243682861328|\n",
      "epoch:5 Training | loss:133.447998046875 score: 6.523109436035156| \n",
      " Validation | loss:143.4901580810547 score:6.620360851287842|\n",
      "epoch:10 Training | loss:127.65852355957031 score: 6.473388195037842| \n",
      " Validation | loss:138.28958129882812 score:6.573629379272461|\n",
      "epoch:15 Training | loss:127.3659896850586 score: 6.470440864562988| \n",
      " Validation | loss:138.302734375 score:6.573244571685791|\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(3,shuffle=True,random_state=42)\n",
    "#kfold\n",
    "for k , (train_idx,valid_idx) in enumerate(kfold.split(train)):\n",
    "    batch_size = 64\n",
    "    epochs = 20\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"{device} is used\")\n",
    "    x_train,x_valid,y_train,y_valid = train[train_idx,:],train[valid_idx,:],target[train_idx],target[valid_idx]\n",
    "    n = x_train.shape[1]\n",
    "    model = Model(n)\n",
    "    model.to(device)\n",
    "    lr = 0.1\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "    train_tensor = torch.tensor(x_train,dtype=torch.float)\n",
    "    y_train_tensor = torch.tensor(y_train,dtype=torch.float)\n",
    "\n",
    "    train_ds = TensorDataset(train_tensor,y_train_tensor)\n",
    "    train_dl = DataLoader(train_ds,\n",
    "                          batch_size = batch_size,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True\n",
    "                          )\n",
    "\n",
    "    valid_tensor = torch.tensor(x_valid,dtype=torch.float)\n",
    "    y_valid_tensor = torch.tensor(y_valid,dtype=torch.float)\n",
    "\n",
    "    valid_ds = TensorDataset(valid_tensor,y_valid_tensor)\n",
    "    valid_dl = DataLoader(valid_ds,\n",
    "                          batch_size = batch_size,\n",
    "                          num_workers=4,\n",
    "                          shuffle=False\n",
    "                          )\n",
    "\n",
    "    print(f\"Fold {k}\")\n",
    "    for i in range(epochs):\n",
    "        losses,metrics = train_loop(train_dl,model,loss_fn,device,optimizer,lr_scheduler)\n",
    "        valid_losses,valid_metrics = valid_loop(valid_dl,model,loss_fn,device)\n",
    "        if (i)%5==0:\n",
    "            print(f\"epoch:{i} Training | loss:{np.mean(losses)} score: {np.mean(metrics)}| \\n Validation | loss:{np.mean(valid_losses)} score:{np.mean(valid_metrics)}|\")\n",
    "    torch.save(model.state_dict(),f'model{k}.bin')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have 3 models, we can do ensemble if we want to predict FVC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}